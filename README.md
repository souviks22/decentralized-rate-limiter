# ðŸŒ Decentralized Rate Limiter

[![Go Report Card](https://goreportcard.com/badge/github.com/souviks22/decentralized-rate-limiter)](https://goreportcard.com/report/github.com/souviks22/decentralized-rate-limiter)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Made with Go](https://img.shields.io/badge/Made%20with-Go-1f425f.svg)](https://golang.org)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/souviks22/decentralized-rate-limiter/issues)

> Most rate limiters assume a central truth.
> This one assumes failure.

This project explores how rate limiting behaves when **central coordination is expensive, unreliable, or undesirable** â€” across regions, failures, and partitions. Instead of enforcing a single global counter, each node makes **local decisions** and reconciles state **eventually**, using CRDTs and peer-to-peer gossip.

The goal is simple but strict:
**never block locally, never trust globally, and still converge.**

---

## Why This Exists

Traditional rate limiters (Redis, centralized gateways) work well â€” until:

* cross-region latency dominates the hot path,
* a single dependency becomes a blast radius,
* or failure handling turns into policy ambiguity.

In large distributed systems, **availability often matters more than precision**.
This rate limiter is designed for those environments.

Each node:

* limits requests **locally**,
* survives **network partitions**,
* and synchronizes state **without a leader**.

Precision is relaxed. Safety and continuity are not.

---

## High-Level Design

At a high level, every node is fully capable of enforcing limits on its own.

```
Client â†’ Any Node â†’ Local Decision â†’ Eventual Reconciliation
```

No node blocks waiting for global state. Synchronization happens **off the hot path**.

---

## How It Works (End-to-End)

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚      Client Request      â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    Peer Node (e.g., Node A)    â”‚
                         â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                         â”‚  1. Receive userID request     â”‚
                         â”‚  2. Check in-memory LRU cache  â”‚
                         â”‚  3. If miss, load from disk    â”‚
                         â”‚  4. Call TokenBucket.consume() â”‚
                         â”‚  5. Add to CRDT delta cache    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                             â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ In-Memory LRU  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Disk Storage (/data) â”‚        â”‚  CRDT Delta Cache  â”‚
â”‚  Token Buckets â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                                       â”‚ libp2p Gossip â”‚
     â”‚                                                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                                              â”‚
     â”‚     Broadcast deltas (every 100ms or 100 entries)            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                 â–¼                                    â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚    Peer Node B      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Peer Node C     â”‚
                                     â”‚  (Same architecture)â”‚     P2P Sync    â”‚  (Same architecture)â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### The important detail

**The request path never waits for gossip.**
If the node is alive, it answers.

---

## Core Components & Why They Exist

### ðŸª£ Token Bucket (Local Authority)

Each user is governed by a standard token bucket:

* capacity for bursts,
* refill rate for sustained traffic.

This is deliberately simple:

* predictable latency,
* constant-time decisions,
* easy to reason about under load.

Thread safety is explicit â€” no hidden concurrency tricks.

---

### ðŸ§  CRDT Synchronization (Global Convergence)

Local decisions generate **deltas**, not full state.

Why deltas?

* smaller payloads,
* less merge ambiguity,
* faster convergence.

CRDT merges are:

* commutative,
* idempotent,
* monotonic.

This guarantees that:

> even if messages are duplicated, delayed, or reordered, nodes eventually agree.

Exact precision is not promised.
**Bounded divergence is.**

---

### ðŸ“¡ libp2p Gossip (Decentralization Without Orchestration)

There is:

* no leader,
* no coordinator,
* no central broker.

Nodes discover peers and exchange updates via libp2p gossip.
Failures are treated as routine, not exceptional.

If a node disappears:

* others continue,
* state reconverges when it returns.

---

### ðŸ’¾ LRU + Disk (Scaling Beyond Memory)

Keeping billions of users in memory is unrealistic.

So the system:

* keeps **hot buckets in an in-memory LRU**,
* evicts cold buckets to **disk**,
* reloads lazily on demand.

This keeps:

* memory bounded,
* hot paths fast,
* cold users cheap.

Durability is pragmatic, not transactional.

---

## Performance Characteristics

Measured on a small libp2p mesh (3 nodes):

| Metric                 | Observation             |
| ---------------------- | ----------------------- |
| Throughput per node    | ~3,000 req/sec          |
| p99 request latency    | ~2 ms                   |
| p99 gossip convergence | ~2 ms                   |
| Gossip payload size    | ~3 KB                   |

These numbers matter less than *where latency lives*:

* request path â†’ local only,
* synchronization â†’ async.

---

## Failure Semantics (Explicit)

This system chooses availability over strict correctness.

* **Network partition** â†’ nodes continue independently
* **Node crash** â†’ local state lost, global state recovers
* **Delayed gossip** â†’ temporary over-allowing possible

This is intentional.

If your use case requires **strict global enforcement**, this is not the right tool.

---

## Example Usage

```go
limiter := drl.NewRateLimiter(100, 10) // capacity, refill rate

if limiter.AllowRequest("user-123") {
    // request proceeds
} else {
    // rate limited locally
}
```

The API stays boring on purpose.
The complexity lives inside.

---

## When You Should (and Shouldnâ€™t) Use This

**Good fit if:**

* low latency matters more than perfect precision,
* regions must operate independently,
* failures are common, not exceptional.

**Not a good fit if:**

* every request must respect a single global counter,
* over-allowing is unacceptable,
* centralized infrastructure is cheap and reliable for you.

---

## Closing Thought

This project is not about replacing Redis.

Itâ€™s about asking a harder question:

> *What does rate limiting look like when the system itself refuses to be centralized?*

If that question matters to you, this project might too.

